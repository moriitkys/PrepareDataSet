{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset by Keras ( - Keras VS Pytorch - )\n",
    "moriitkys\n",
    "\n",
    "# <font color=\"OrangeRed\">データセットのバックアップを取ってから実行してください</font>\n",
    "## Make a backup of your dataset before running this program\n",
    "\n",
    "KerasやPyTorchで学習をする際のパラメータ設定ＵＩとデータセット準備ができます。   \n",
    "File Directory　クラスごとにフォルダを作り、その中に該当クラスの画像を全て入れる(<font color=\"OrangeRed\">データがそれぞれ10以下の場合エラーが出る可能性あり</font>)   \n",
    "MyOwnNN/dataset/1/img0001.png, img0002.png, ...   \n",
    "MyOwnNN/dataset/2/img0001.png, img0002.png, ...   \n",
    "- 1や2はクラス名で、HookWrenchやSpannerWrenchという名前でもOK   \n",
    "- クラスのラベルとデータパスの対応を示したtxtやcsvのようなファイルは必要なく、データの入ったフォルダから自動でクラス名を取得してcategoriesに保持し対応付けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilenet\n",
      "img_size=[192, 192]\n",
      "train mode\n",
      "total epochs = 55\n",
      "splitting complete\n",
      "Now executing augmentation :dataset/HookWrench\n",
      "Now executing augmentation :dataset/SpannerWrench\n",
      "Now executing augmentation :dataset_val/HookWrench\n",
      "Now executing augmentation :dataset_val/SpannerWrench\n",
      "dataset source is dataset_aug&dataset_val_aug\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#Settings and prepare your dataset\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, optimizers\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.preprocessing.image import random_rotation, random_shift, random_zoom\n",
    "\n",
    "#numpy==1.18.4 ->  numpy-1.16.4 (管理者権限でAnaconda Prompt)\n",
    "#h5py==  -> 2.8.0rc1 (pip install h5py==2.8.0rc1)\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mylib.makedataset_rgb as mkdataset\n",
    "import mylib.create_panel as create_panel\n",
    "import mylib.utils as myutils\n",
    "\n",
    "# ------ Setting panels ------\n",
    "import tkinter\n",
    "from tkinter import messagebox\n",
    "img_size_mynet = [28,28]# You can change input image size(Pay attention to network shape)\n",
    "setting_panel = create_panel.CreatePanel(img_size_mynet = img_size_mynet)\n",
    "setting_panel.create_buttons()#If you push \"start\", exit this line.\n",
    "\n",
    "# ------ set params and preparing dataset ------\n",
    "flag_train = setting_panel.flag_train\n",
    "flag_aug = setting_panel.flag_aug\n",
    "flag_split = setting_panel.flag_split\n",
    "ratio_train = float(setting_panel.var_sp.get())#0.0 ~ 1.0\n",
    "total_epochs = int(setting_panel.var_sp_epochs.get())\n",
    "\n",
    "type_backbone = setting_panel.type_backbone#ex) ResNet, Mobilenet, MyNet\n",
    "layer_name_gradcam = setting_panel.layer_name_gradcam# Don't use \n",
    "img_size = setting_panel.img_size#ex) ResNet:[224,224], Mobilenet:[192,192], MyNet:[28,28]\n",
    "print(type_backbone)\n",
    "print(\"img_size=\" + str(img_size))\n",
    "\n",
    "#How many classes are in \"dataset\" folder\n",
    "categories = [i for i in os.listdir(os.getcwd().replace(\"/mylib\", \"\") + \"/dataset\")]\n",
    "categories_idx = {}#ex) HookWrench:0, SpannerWrench:1\n",
    "for i, name in enumerate(categories):\n",
    "    categories_idx[name] = i\n",
    "nb_classes = len(categories)#ex) nb_classes=2\n",
    "\n",
    "dirname_dataset = \"dataset\"# dataset folder\n",
    "dirname_dataset_val = dirname_dataset + \"_val\"\n",
    "output_folder = \"outputs_keras/\"+type_backbone\n",
    "x_train, y_train, x_val, y_val = [],[],[],[]\n",
    "\n",
    "def aug_dataset(dirname_dataset_1, dirname_dataset_val_1):\n",
    "    '''\n",
    "    This function returns updated dataset dirname \n",
    "    Contain MakeDatasetRGB() (mylib/makedataset_rgb.py)\n",
    "    Argument1: Foldername (String), Argument2: Foldername (String)\n",
    "    Usage:\n",
    "    dirname_dataset, dirname_dataset_val = aug_dataset(dirname_dataset, dirname_dataset_val)\n",
    "    '''\n",
    "    dirname_dataset_aug = dirname_dataset_1 + \"_aug\"\n",
    "    dirname_dataset_val_aug = dirname_dataset_val_1 + \"_aug\"\n",
    "    make_dataset = mkdataset.MakeDataSetRGB()\n",
    "    if os.path.exists(dirname_dataset_aug ) == True \\\n",
    "    or os.path.exists(dirname_dataset_val_aug ) == True:\n",
    "        #https://pythonbasics.org/tkinter-messagebox/\n",
    "        tki2 = tkinter.Tk()\n",
    "        tki2.withdraw()\n",
    "        ret = messagebox.askyesno('確認', '_augフォルダがあります。_augフォルダ内を消去してよろしいですか？')\n",
    "        if ret == True:\n",
    "            if os.path.exists(dirname_dataset_aug ) == True:\n",
    "                shutil.rmtree(dirname_dataset_aug)\n",
    "            if os.path.exists(dirname_dataset_val_aug ) == True:\n",
    "                shutil.rmtree(dirname_dataset_val_aug)\n",
    "            make_dataset.do_augmentation(dataset_folder_name = \"dataset\")\n",
    "            make_dataset.do_augmentation(dataset_folder_name = \"dataset_val\")\n",
    "            tki2.destroy()\n",
    "        else:\n",
    "            tki2.destroy()\n",
    "        tki2.mainloop()\n",
    "    else:\n",
    "        make_dataset.do_augmentation(dataset_folder_name = \"dataset\")\n",
    "        make_dataset.do_augmentation(dataset_folder_name = \"dataset_val\")\n",
    "        \n",
    "    dirname_dataset_2 = dirname_dataset_1 + \"_aug\"\n",
    "    dirname_dataset_val_2 = dirname_dataset_val_1 + \"_aug\"\n",
    "    return dirname_dataset_2, dirname_dataset_val_2\n",
    "\n",
    "def prepare_dataset_val():\n",
    "    for j in categories:\n",
    "        if os.path.exists(dirname_dataset_val  + \"\\\\\" + str(j) ) == False:\n",
    "            os.makedirs(dirname_dataset_val + \"\\\\\" + str(j))\n",
    "            files = glob.glob(dirname_dataset + \"\\\\\" + str(j) + \"/*\")\n",
    "            for imgfile in files:# move some data from \"dataset\" to \"dataset_val\"\n",
    "                if myutils.train_or_val(ratio_train) == \"val\":\n",
    "                    shutil.move(imgfile, dirname_dataset_val+\"\\\\\" + str(j) + \"/\")\n",
    "\n",
    "def revert_dataset_val():\n",
    "    '''\n",
    "    Revert Dataset (\"dataset\" & \"dataset_val\" -> \"dataset\")\n",
    "    This function revert splitted validation dataset directory to dataset directory\n",
    "    '''\n",
    "    for j in categories:\n",
    "        if os.path.exists(dirname_dataset_val  + \"\\\\\" + str(j) ) == True:\n",
    "            files = glob.glob(dirname_dataset_val + \"\\\\\" + str(j) + \"/*\")\n",
    "            for imgfile in files:#Move all images in \"dataset_val\" to \"dataset\"\n",
    "                shutil.move(imgfile, dirname_dataset + \"\\\\\" + str(j))\n",
    "    if os.path.exists(dirname_dataset_val) == True:\n",
    "        shutil.rmtree(dirname_dataset_val)#Delete \"dataset_val\" folder\n",
    "\n",
    "def prepare_dataset(dirname_dataset, dirname_dataset_val):\n",
    "    label = 0\n",
    "    for j in categories:# Prepare Training Dataset\n",
    "        files = glob.glob(dirname_dataset + \"\\\\\" + str(j) + \"/*\")\n",
    "        for imgfile in files:\n",
    "            img = load_img(imgfile, target_size=(img_size[0], img_size[1]))\n",
    "            array = img_to_array(img) / 255\n",
    "            x_train.append(array)\n",
    "            y_train.append(label)\n",
    "        label += 1\n",
    "\n",
    "    label = 0\n",
    "    for j in categories:# Prepare Validation Dataset\n",
    "        files = glob.glob(dirname_dataset_val + \"\\\\\" + str(j) + \"/*\")\n",
    "        for imgfile in files:\n",
    "            img = load_img(imgfile, target_size=(img_size[0], img_size[1]))\n",
    "            array = img_to_array(img) / 255\n",
    "            x_val.append(array)\n",
    "            y_val.append(label)\n",
    "        label += 1\n",
    "            \n",
    "if flag_train == True:\n",
    "    print(\"train mode\")\n",
    "    print(\"total epochs = \" + str(total_epochs))\n",
    "    if flag_split == True:\n",
    "        revert_dataset_val()\n",
    "        prepare_dataset_val()\n",
    "        print(\"splitting complete\")\n",
    "    if flag_split == False and os.path.exists(dirname_dataset_val) == False:\n",
    "        prepare_dataset_val()\n",
    "        print(\"You have not splitted dataset, so splitteing automatically done\")\n",
    "    if flag_aug == True:\n",
    "        dirname_dataset, dirname_dataset_val = aug_dataset(dirname_dataset, dirname_dataset_val)\n",
    "        print(\"dataset source is \" + dirname_dataset + \"&\" + dirname_dataset_val)\n",
    "    elif flag_aug == False:\n",
    "        dirname_dataset_aug = dirname_dataset + \"_aug\"\n",
    "        dirname_dataset_val_aug = dirname_dataset_val + \"_aug\"\n",
    "        make_dataset = mkdataset.MakeDataSetRGB()\n",
    "        if os.path.exists(dirname_dataset_aug ) == True \\\n",
    "        and os.path.exists(dirname_dataset_val_aug ) == True:\n",
    "            dirname_dataset = dirname_dataset_aug\n",
    "            dirname_dataset_val = dirname_dataset_val_aug\n",
    "    prepare_dataset(dirname_dataset, dirname_dataset_val)\n",
    "    # make directory (weights_folder, outputs)\n",
    "    if os.path.exists(\"weights_pytorch/\"+type_backbone) == False:\n",
    "        os.makedirs(\"weights_pytorch/\"+type_backbone)\n",
    "    if os.path.exists(\"outputs_pytorch/\"+type_backbone) == False:\n",
    "        os.makedirs(\"outputs_pytorch/\"+type_backbone)\n",
    "        \n",
    "if os.path.exists(output_folder) == False:\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# In Keras, use numpy array for NN model\n",
    "if os.path.exists(\"tmp_npy\") == False:\n",
    "    os.makedirs(\"tmp_npy\")\n",
    "x_train, y_train, x_val, y_val = np.array(x_train), np.array(y_train), np.array(x_val), np.array(y_val)\n",
    "np.save(\"tmp_npy/x_train.npy\", x_train)\n",
    "np.save(\"tmp_npy/y_train.npy\", y_train)\n",
    "np.save(\"tmp_npy/x_test.npy\", x_val)\n",
    "np.save(\"tmp_npy/y_test.npy\", y_val)\n",
    "x_train, y_train, x_val, y_val = [],[],[],[]\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下はdataset_valのデータをdatasetに統合するプログラムなので、必要な時以外に実行しないでください。   \n",
    "Don't use the next cell except merging dataset_val and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HookWrench', 'SpannerWrench']\n"
     ]
    }
   ],
   "source": [
    "# Revert Dataset (dataset & dataset_val -> dataset)\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "#categories = [i for i in os.listdir(os.getcwd().replace(\"/mylib\", \"\") + \"/dataset\")]\n",
    "categories = [i for i in os.listdir(os.getcwd()+ \"/dataset\")]\n",
    "print(categories)\n",
    "dirname_dataset = \"dataset\"# dataset folder\n",
    "dirname_dataset_val = dirname_dataset + \"_val\" # validation dataset folder\n",
    "\n",
    "def revert_dataset_val():\n",
    "    '''\n",
    "    Revert Dataset\n",
    "    This function revert splitted validation dataset directory to dataset directory\n",
    "    '''\n",
    "    for j in categories:\n",
    "        if os.path.exists(dirname_dataset_val  + \"\\\\\" + str(j) ) == True:\n",
    "            files = glob.glob(dirname_dataset_val + \"\\\\\" + str(j) + \"/*\")\n",
    "            for imgfile in files:\n",
    "                shutil.move(imgfile, dirname_dataset + \"\\\\\" + str(j))\n",
    "    if os.path.exists(dirname_dataset_val) == True:\n",
    "        shutil.rmtree(dirname_dataset_val)\n",
    "revert_dataset_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 実行環境\n",
    "- Windows10\n",
    "- CPU:Core i7-7700HQ\n",
    "- Memory: 16GB\n",
    "- Graphic board: GTX1060 6GB\n",
    "- Strage: NVMe M.2 SSD 1TB\n",
    "- CUDA 9.0.176   \n",
    "- cuDNN 7.0.5   \n",
    "\n",
    "\n",
    "- Keras==2.1.5\n",
    "- tensorflow-gpu==1.11.0\n",
    "- torch==1.1.0\n",
    "- scikit-learn==0.19.1\n",
    "- scipy==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
